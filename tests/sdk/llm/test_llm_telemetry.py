import json
import os
import tempfile
import time
import warnings
from unittest.mock import MagicMock, patch

import pytest
from litellm.types.utils import ModelResponse, Usage
from pydantic import ValidationError

from openhands.sdk.llm.utils.metrics import Metrics
from openhands.sdk.llm.utils.telemetry import Telemetry, _safe_json


@pytest.fixture
def mock_metrics():
    """Create a mock Metrics instance."""
    return Metrics()


@pytest.fixture
def basic_telemetry(mock_metrics):
    """Create a basic Telemetry instance for testing."""
    return Telemetry(model_name="gpt-4o", log_enabled=False, metrics=mock_metrics)


@pytest.fixture
def mock_response():
    """Create a mock ModelResponse for testing."""
    return ModelResponse(
        id="test-response-id",
        choices=[],
        created=1234567890,
        model="gpt-4o",
        object="chat.completion",
        usage=Usage(prompt_tokens=100, completion_tokens=50, total_tokens=150),
    )


class TestTelemetryInitialization:
    """Test Telemetry class initialization and configuration."""

    def test_telemetry_default_initialization(self, mock_metrics):
        """Test Telemetry initialization with default values."""
        telemetry = Telemetry(metrics=mock_metrics)

        assert telemetry.model_name == "unknown"
        assert telemetry.log_enabled is False
        assert telemetry.log_dir is None
        assert telemetry.input_cost_per_token is None
        assert telemetry.output_cost_per_token is None
        assert telemetry.metrics == mock_metrics

    def test_telemetry_custom_initialization(self, mock_metrics):
        """Test Telemetry initialization with custom values."""
        telemetry = Telemetry(
            model_name="custom-model",
            log_enabled=True,
            log_dir="/tmp/logs",
            input_cost_per_token=0.001,
            output_cost_per_token=0.002,
            metrics=mock_metrics,
        )

        assert telemetry.model_name == "custom-model"
        assert telemetry.log_enabled is True
        assert telemetry.log_dir == "/tmp/logs"
        assert telemetry.input_cost_per_token == 0.001
        assert telemetry.output_cost_per_token == 0.002
        assert telemetry.metrics == mock_metrics

    def test_telemetry_validation_error(self):
        """Test that Telemetry raises ValidationError when metrics is missing."""
        with pytest.raises(ValidationError):
            Telemetry()  # type: ignore

    def test_telemetry_private_attributes(self, basic_telemetry):
        """Test that private attributes are initialized correctly."""
        # Private attributes should be accessible but not serialized
        assert hasattr(basic_telemetry, "_req_start")
        assert hasattr(basic_telemetry, "_req_ctx")
        assert hasattr(basic_telemetry, "_last_latency")

        # Check default values
        assert basic_telemetry._req_start == 0.0
        assert basic_telemetry._req_ctx == {}
        assert basic_telemetry._last_latency == 0.0


class TestTelemetryLifecycle:
    """Test Telemetry lifecycle methods."""

    def test_on_request_basic(self, basic_telemetry):
        """Test on_request method with basic functionality."""
        start_time = time.time()
        basic_telemetry.on_request(None)

        # Should set request start time
        assert basic_telemetry._req_start >= start_time
        assert basic_telemetry._req_ctx == {}

    def test_on_request_with_context(self, basic_telemetry):
        """Test on_request method with log context."""
        log_ctx = {"context_window": 4096, "user_id": "test-user"}
        basic_telemetry.on_request(log_ctx)

        assert basic_telemetry._req_ctx == log_ctx

    def test_on_error_stub(self, basic_telemetry):
        """Test on_error method (currently a stub)."""
        # Should not raise any exceptions
        basic_telemetry.on_error(Exception("test error"))

    @patch("time.time")
    def test_on_response_latency_tracking(
        self, mock_time, basic_telemetry, mock_response
    ):
        """Test that on_response correctly tracks latency."""
        # Set up time sequence
        mock_time.side_effect = [1000.0, 1002.5]  # 2.5 second latency

        basic_telemetry.on_request(None)
        metrics = basic_telemetry.on_response(mock_response)

        assert basic_telemetry._last_latency == 2.5
        assert isinstance(metrics.accumulated_cost, float)

    def test_on_response_with_usage(self, basic_telemetry):
        """Test on_response with usage information."""
        basic_telemetry.on_request({"context_window": 4096})

        # Create a ModelResponse with usage data
        response = ModelResponse(
            id="test-response-id",
            usage=Usage(prompt_tokens=100, completion_tokens=50, total_tokens=150),
        )

        basic_telemetry.on_response(response)

        # Should record token usage
        assert len(basic_telemetry.metrics.token_usages) == 1
        token_usage = basic_telemetry.metrics.token_usages[0]
        assert token_usage.prompt_tokens == 100
        assert token_usage.completion_tokens == 50


class TestTelemetryTokenUsage:
    """Test token usage recording functionality."""

    def test_record_usage_basic(self, basic_telemetry):
        """Test basic token usage recording."""
        usage = Usage(prompt_tokens=100, completion_tokens=50, total_tokens=150)

        basic_telemetry._record_usage(usage, "test-id", 4096)

        assert len(basic_telemetry.metrics.token_usages) == 1
        token_usage = basic_telemetry.metrics.token_usages[0]
        assert token_usage.prompt_tokens == 100
        assert token_usage.completion_tokens == 50
        assert token_usage.cache_read_tokens == 0
        assert token_usage.cache_write_tokens == 0
        assert token_usage.context_window == 4096
        assert token_usage.response_id == "test-id"

    def test_record_usage_with_cache_read(self, basic_telemetry):
        """Test token usage recording with cache read tokens."""
        # Create a mock usage with prompt_tokens_details
        usage = Usage(prompt_tokens=100, completion_tokens=50, total_tokens=150)

        # Mock the prompt_tokens_details attribute
        mock_details = MagicMock()
        mock_details.cached_tokens = 25
        usage.prompt_tokens_details = mock_details

        basic_telemetry._record_usage(usage, "test-id", 4096)

        token_usage = basic_telemetry.metrics.token_usages[0]
        assert token_usage.cache_read_tokens == 25

    def test_record_usage_with_cache_write(self, basic_telemetry):
        """Test token usage recording with cache write tokens."""
        from litellm import Usage

        usage = Usage.model_construct(
            prompt_tokens=100,
            completion_tokens=50,
            total_tokens=150,
            model_extra={"cache_creation_input_tokens": 30},
        )
        # Set the attribute that telemetry code expects
        usage._cache_creation_input_tokens = 30

        basic_telemetry._record_usage(usage, "test-id", 4096)

        token_usage = basic_telemetry.metrics.token_usages[0]
        assert token_usage.cache_write_tokens == 30

    def test_record_usage_missing_tokens(self, basic_telemetry):
        """Test token usage recording with missing token counts."""
        usage = Usage()  # Empty usage

        basic_telemetry._record_usage(usage, "test-id", 4096)

        token_usage = basic_telemetry.metrics.token_usages[0]
        assert token_usage.prompt_tokens == 0
        assert token_usage.completion_tokens == 0


class TestTelemetryCostCalculation:
    """Test cost calculation functionality."""

    def test_compute_cost_with_custom_rates(self, mock_metrics):
        """Test cost computation with custom input/output rates."""
        telemetry = Telemetry(
            model_name="gpt-4o",
            input_cost_per_token=0.001,
            output_cost_per_token=0.002,
            metrics=mock_metrics,
        )

        mock_response = ModelResponse(
            id="test-id",
            choices=[],
            created=1234567890,
            model="gpt-4o",
            object="chat.completion",
            usage=Usage(prompt_tokens=100, completion_tokens=50, total_tokens=150),
        )

        with patch(
            "openhands.sdk.llm.utils.telemetry.litellm_completion_cost"
        ) as mock_cost:
            mock_cost.return_value = 0.25
            telemetry._compute_cost(mock_response)

            # Should call litellm with custom cost per token
            mock_cost.assert_called_once()
            call_kwargs = mock_cost.call_args[1]
            assert "custom_cost_per_token" in call_kwargs
            # CostPerToken is a TypedDict, so check it has the expected keys
            cost_per_token = call_kwargs["custom_cost_per_token"]
            assert "input_cost_per_token" in cost_per_token
            assert "output_cost_per_token" in cost_per_token

    def test_compute_cost_from_headers(self, basic_telemetry):
        """Test cost extraction from response headers."""
        mock_response = MagicMock()
        mock_response._hidden_params = {
            "additional_headers": {"llm_provider-x-litellm-response-cost": "0.15"}
        }

        cost = basic_telemetry._compute_cost(mock_response)
        assert cost == 0.15

    def test_compute_cost_litellm_fallback(self, basic_telemetry):
        """Test fallback to litellm cost calculator."""
        mock_response = ModelResponse(
            id="test-id",
            choices=[],
            created=1234567890,
            model="gpt-4o",
            object="chat.completion",
        )

        with patch(
            "openhands.sdk.llm.utils.telemetry.litellm_completion_cost"
        ) as mock_cost:
            mock_cost.return_value = 0.30
            cost = basic_telemetry._compute_cost(mock_response)

            assert cost == 0.30
            mock_cost.assert_called_once()

    def test_compute_cost_failure_handling(self, basic_telemetry):
        """Test cost calculation failure handling."""
        mock_response = ModelResponse(
            id="test-id",
            choices=[],
            created=1234567890,
            model="gpt-4o",
            object="chat.completion",
        )

        with patch(
            "openhands.sdk.llm.utils.telemetry.litellm_completion_cost"
        ) as mock_cost:
            mock_cost.side_effect = Exception("Cost calculation failed")

            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter("always")
                cost = basic_telemetry._compute_cost(mock_response)

                assert cost is None
                assert len(w) == 1
                assert "Cost calculation failed" in str(w[0].message)

    def test_compute_cost_model_name_processing(self, mock_metrics):
        """Test that model name is processed correctly for litellm."""
        telemetry = Telemetry(model_name="provider/gpt-4o-mini", metrics=mock_metrics)

        mock_response = ModelResponse(
            id="test-id",
            choices=[],
            created=1234567890,
            model="gpt-4o-mini",
            object="chat.completion",
        )

        with patch(
            "openhands.sdk.llm.utils.telemetry.litellm_completion_cost"
        ) as mock_cost:
            mock_cost.return_value = 0.10
            telemetry._compute_cost(mock_response)

            # Should strip provider prefix
            call_kwargs = mock_cost.call_args[1]
            assert call_kwargs["model"] == "gpt-4o-mini"


class TestTelemetryLogging:
    """Test telemetry logging functionality."""

    def test_log_completion_disabled(self, basic_telemetry, mock_response):
        """Test that logging is skipped when disabled."""
        basic_telemetry.on_request({"test": "context"})

        # Should not create any files when log_enabled is False
        with tempfile.TemporaryDirectory() as temp_dir:
            basic_telemetry.log_dir = temp_dir
            # Use on_response instead of _log_completion directly to test the full flow
            basic_telemetry.on_response(mock_response)

            # No files should be created since logging is disabled
            assert len(os.listdir(temp_dir)) == 0

    def test_log_completion_no_directory(self, mock_metrics, mock_response):
        """Test logging when no log directory is set."""
        telemetry = Telemetry(
            model_name="gpt-4o", log_enabled=True, log_dir=None, metrics=mock_metrics
        )

        # Should return early without error
        telemetry.log_llm_call(mock_response, 0.25)

    def test_log_completion_success(self, mock_metrics, mock_response):
        """Test successful completion logging."""
        with tempfile.TemporaryDirectory() as temp_dir:
            telemetry = Telemetry(
                model_name="gpt-4o",
                log_enabled=True,
                log_dir=temp_dir,
                metrics=mock_metrics,
            )

            # Set up context and latency
            telemetry.on_request({"user_id": "test-user", "context_window": 4096})
            telemetry._last_latency = 1.5

            telemetry.log_llm_call(mock_response, 0.25)

            # Should create a log file
            files = os.listdir(temp_dir)
            assert len(files) == 1

            # Check file content
            with open(os.path.join(temp_dir, files[0])) as f:
                data = json.loads(f.read())

            assert data["user_id"] == "test-user"
            assert data["context_window"] == 4096
            assert data["cost"] == 0.25
            assert data["latency_sec"] == 1.5
            assert "response" in data
            assert "timestamp" in data

    def test_log_completion_with_raw_response(self, mock_metrics, mock_response):
        """Test logging with raw response included."""
        with tempfile.TemporaryDirectory() as temp_dir:
            telemetry = Telemetry(
                model_name="gpt-4o",
                log_enabled=True,
                log_dir=temp_dir,
                metrics=mock_metrics,
            )

            raw_response = ModelResponse(
                id="raw-id",
                choices=[],
                created=1234567890,
                model="gpt-4o",
                object="chat.completion",
            )

            telemetry.on_request({})
            telemetry.log_llm_call(mock_response, 0.25, raw_resp=raw_response)

            files = os.listdir(temp_dir)
            with open(os.path.join(temp_dir, files[0])) as f:
                data = json.loads(f.read())

            assert "raw_response" in data

    def test_log_completion_model_name_sanitization(self, mock_metrics, mock_response):
        """Test that model names with slashes are sanitized in filenames."""
        with tempfile.TemporaryDirectory() as temp_dir:
            telemetry = Telemetry(
                model_name="provider/gpt-4o",
                log_enabled=True,
                log_dir=temp_dir,
                metrics=mock_metrics,
            )

            telemetry.on_request({})
            telemetry.log_llm_call(mock_response, 0.25)

            files = os.listdir(temp_dir)
            assert len(files) == 1
            # Should replace '/' with '__'
            assert "provider__gpt-4o" in files[0]

    def test_log_completion_error_handling(self, mock_metrics, mock_response):
        """Test logging error handling."""
        # Use a guaranteed-invalid log_dir by pointing at a regular file path
        # rather than a directory. This avoids reliance on environment-specific
        # directories that may unexpectedly exist or be writable in CI.
        tmp = tempfile.NamedTemporaryFile(delete=False)
        try:
            bogus_path = tmp.name
            telemetry = Telemetry(
                model_name="gpt-4o",
                log_enabled=True,
                log_dir=bogus_path,
                metrics=mock_metrics,
            )

            telemetry.on_request({})

            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter("always")
                telemetry.log_llm_call(mock_response, 0.25)

                # Should issue a warning but not crash
                assert len(w) == 1
                assert "Telemetry logging failed" in str(w[0].message)
        finally:
            try:
                tmp.close()
            except Exception:
                pass
            try:
                os.unlink(tmp.name)
            except Exception:
                pass


class TestTelemetryIntegration:
    """Test full telemetry integration scenarios."""

    def test_full_request_response_cycle(self, mock_metrics):
        """Test complete request-response cycle with all features."""
        with tempfile.TemporaryDirectory() as temp_dir:
            telemetry = Telemetry(
                model_name="gpt-4o",
                log_enabled=True,
                log_dir=temp_dir,
                input_cost_per_token=0.001,
                output_cost_per_token=0.002,
                metrics=mock_metrics,
            )

            # Start request
            log_ctx = {"user_id": "test-user", "context_window": 4096}
            telemetry.on_request(log_ctx)

            # Create response with usage (ModelResponse format)
            response = ModelResponse(
                id="test-response-id",
                usage=Usage(prompt_tokens=100, completion_tokens=50, total_tokens=150),
            )

            with patch(
                "openhands.sdk.llm.utils.telemetry.litellm_completion_cost"
            ) as mock_cost:
                mock_cost.return_value = 0.25
                metrics = telemetry.on_response(response)  # type: ignore

            # Verify all aspects
            assert metrics.accumulated_cost == 0.25
            assert len(telemetry.metrics.token_usages) == 1
            assert len(telemetry.metrics.costs) == 1
            assert len(telemetry.metrics.response_latencies) == 1

            # Verify log file was created
            files = os.listdir(temp_dir)
            assert len(files) == 1

    def test_multiple_requests(self, basic_telemetry):
        """Test handling multiple sequential requests."""
        responses = []

        for i in range(3):
            basic_telemetry.on_request({"request_id": i})

            response = ModelResponse(
                id=f"response-{i}",
                usage=Usage(
                    prompt_tokens=100 + i * 10,
                    completion_tokens=50 + i * 5,
                    total_tokens=150 + i * 15,
                ),
            )

            with patch(
                "openhands.sdk.llm.utils.telemetry.litellm_completion_cost"
            ) as mock_cost:
                mock_cost.return_value = 0.1 + i * 0.05
                cost = basic_telemetry.on_response(response)
                responses.append((response, cost))

        # Should have recorded all requests
        assert len(basic_telemetry.metrics.token_usages) == 3
        assert len(basic_telemetry.metrics.costs) == 3
        assert len(basic_telemetry.metrics.response_latencies) == 3

        # Verify accumulated metrics
        total_cost = sum(cost.cost for cost in basic_telemetry.metrics.costs)
        assert abs(total_cost - 0.45) < 1e-10  # Handle floating point precision


class TestSafeJsonFunction:
    """Test the _safe_json utility function."""

    def test_safe_json_with_dict_object(self):
        """Test _safe_json with object that has __dict__."""

        class TestObj:
            def __init__(self):
                self.attr1 = "value1"
                self.attr2 = 42

        obj = TestObj()
        result = _safe_json(obj)

        assert result == {"attr1": "value1", "attr2": 42}

    def test_safe_json_without_dict(self):
        """Test _safe_json with object that doesn't have __dict__."""
        obj = 42
        result = _safe_json(obj)

        assert result == "42"

    def test_safe_json_with_exception(self):
        """Test _safe_json when __dict__ access raises exception."""

        class BadObj:
            def __getattribute__(self, name):  # type: ignore
                if name == "__dict__":
                    raise Exception("Cannot access __dict__")
                return super().__getattribute__(name)

        obj = BadObj()
        result = _safe_json(obj)

        # Should fall back to str()
        assert isinstance(result, str)


class TestTelemetryEdgeCases:
    """Test edge cases and error conditions."""

    def test_on_response_without_on_request(self, basic_telemetry, mock_response):
        """Test on_response called without prior on_request."""
        # Should not crash, should use current time for latency calculation
        metrics = basic_telemetry.on_response(mock_response)

        assert isinstance(metrics.accumulated_cost, float)
        # Latency might be very small or even negative due to timing precision
        # The important thing is that it doesn't crash
        assert isinstance(basic_telemetry._last_latency, float)

    def test_response_id_extraction_edge_cases(self, basic_telemetry):
        """Test response ID extraction from various response formats."""
        # Test with ModelResponse with ID
        response_with_id = ModelResponse(id="model-response-id", usage=None)
        basic_telemetry.on_request({})
        basic_telemetry.on_response(response_with_id)

        # Test with ModelResponse missing ID
        response_no_id = ModelResponse(usage=None)
        basic_telemetry.on_request({})
        basic_telemetry.on_response(response_no_id)

        # Test with non-ModelResponse object
        with pytest.raises(ValidationError):
            mock_response = MagicMock()
            basic_telemetry.on_request({})
            basic_telemetry.on_response(mock_response)

        # Should have recorded latencies for all cases
        assert len(basic_telemetry.metrics.response_latencies) == 2

    def test_usage_extraction_edge_cases(self, basic_telemetry):
        """Test usage extraction from various response formats."""
        # Test with dict response containing usage
        response = ModelResponse(
            id="test-id",
            usage={
                "prompt_tokens": 100,
                "completion_tokens": 50,
                "total_tokens": 150,
            },
        )

        basic_telemetry.on_request({"context_window": 4096})
        basic_telemetry.on_response(response)
        assert len(basic_telemetry.metrics.token_usages) == 1

        # Test with dict response without usage
        response_no_usage = ModelResponse(id="no-usage-id", usage=None)
        basic_telemetry.on_request({})
        basic_telemetry.on_response(response_no_usage)

        # Should still have only one token usage record
        assert len(basic_telemetry.metrics.token_usages) == 1

    def test_cost_calculation_with_zero_cost(self, basic_telemetry, mock_response):
        """Test cost calculation when cost is zero or None."""
        with patch.object(basic_telemetry, "_compute_cost", return_value=None):
            metrics = basic_telemetry.on_response(mock_response)

            assert metrics.accumulated_cost == 0.0
            # Should not add to costs list when cost is None
            assert len(basic_telemetry.metrics.costs) == 0

        with patch.object(basic_telemetry, "_compute_cost", return_value=0.0):
            metrics = basic_telemetry.on_response(mock_response)

            assert metrics.accumulated_cost == 0.0
            # Should NOT add zero cost to costs list (0.0 is falsy)
            assert len(basic_telemetry.metrics.costs) == 0
